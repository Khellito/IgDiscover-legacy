# kate: syntax Python;
"""
These are the main files that the pipeline creates, in the order in which they
are created:

reads/merged.fastq.gz -- merged reads
reads/trimmed.fastq.gz -- primers removed from merged reads
reads/filtered.fasta  -- too short sequences removed, converted to FASTA
reads/unique.fasta -- collapsed sequences (duplicates removed)
unique.igblast.txt.gz -- IgBLAST output
unique.assigned.tab -- parsed IgBLAST output as a tab-delimited table
unique.filtered.tab -- filtered version of the above
groups.tab -- sequences grouped by barcode
consensus.fasta -- contains one consensus sequence for each group
consensus.igblast.txt -- consensus sequences sent through IgBLAST
consensus.assigned.tab -- parsed IgBLAST output
"""
import yaml
from sqt.dna import reverse_complement
from sqt import FastaReader
import igypipe
from igypipe.utils import relative_symlink

class Config:
	def __init__(self, path):
		# Set some defaults.
		self.merge_program = 'pear'
		self.limit = None  # or an integer
		self.cluster_program = 'vsearch'
		self.multialign_program = 'muscle-fast'
		self.maximum_expected_errors = None  # or an integer
		self.minimum_merged_read_length = 300
		self.mismatch_penalty = None
		self.barcode_length = 0
		self.iterations = 1
		self.ignore_j = False
		self.subsample = 500
		self.stranded = False
		self.forward_primers = None
		self.reverse_primers = None
		self.library_name = os.path.basename(os.getcwd())
		self.read_from(path)

	def read_from(self, path):
		with open(path) as f:
			content = f.read()
		# Heuristic to detect whether the configuration is YAML or in the old format
		# Will be removed after a transition period
		if 'species:' in content:
			# Itâ€™s YAML
			self.__dict__.update(yaml.safe_load(content))
		else:
			exec(content, self.__dict__)
			# Convert all-uppercase attributes to lowercase
			for attribute, value in list(vars(self).items()):
				if attribute == attribute.upper():
					setattr(self, attribute.lower(), value)
			if hasattr(self, 'trim_primers'):
				if self.trim_primers:
					if not self.reverse_primers:
						sys.exit("The list of reverse_primers is empty. This will not work correctly.")
					if not self.forward_primers:
						sys.exit("The list of forward_primers is empty. This will not work correctly.")
				else:
					self.reverse_primers = None
					self.forward_primers = None
				del self.trim_primers

try:
	config = Config('igypipe.yaml')
except FileNotFoundError:
	# For backwards compatibility, try old path also
	try:
		config = Config('pipeline.conf')
	except FileNotFoundError:
		sys.exit("Pipeline configuration file 'igypipe.yaml' not found. Please create it!")

print('IgY-Pipe configuration:')
for k, v in sorted(vars(config).items()):
	# TODO the following line is only necessary for non-YAML configurations
	if k.startswith('_'):
		continue
	print('   ', k, ': ', repr(v), sep='')

# This command is run before every shell command and helps to catch errors early
shell.prefix("set -euo pipefail;")

if config.limit:
	READS1 = 'reads/limited-{}.1.fastq'.format(config.limit)
	READS2 = 'reads/limited-{}.2.fastq'.format(config.limit)
else:
	READS1 = 'reads/decompressed.1.fastq'
	READS2 = 'reads/decompressed.2.fastq'


MERGED_READS = 'reads/merged.fastq.gz'
UNIQUE_READS = 'reads/unique.fasta'


# Targets for each iteration
ITERATION_TARGETS = [
	'clusterplots/done',
	'counts.txt',
	'unique.correlationVJ.pdf',
	'unique.errorhistograms.pdf',
	'unique.V_usage.tab',
	'unique.V_usage.pdf',
	'V_dendrogram.pdf',
]
# Targets for non-final iterations
DISCOVERY_TARGETS = [
	'candidates.tab',
	'new_V_database.fasta',
]
# Targets for final iteration
FINAL_TARGETS = [
	'consensus.correlationVJ.pdf',
	'consensus.V_usage.tab',
	'consensus.V_usage.pdf',
]
TARGETS = expand('iteration-{nr:02d}/{path}', nr=range(1, config.iterations+1), path=ITERATION_TARGETS+DISCOVERY_TARGETS)
TARGETS += expand('final/{path}', path=ITERATION_TARGETS+FINAL_TARGETS)
TARGETS += ['stats/barcodes.txt', 'stats/unique.readlengths.pdf', 'stats/merged.readlengths.pdf']

# Add also intermediate files
TARGETS += [READS1, READS2, MERGED_READS, UNIQUE_READS]

rule all:
	input:
		TARGETS


if config.limit:
	rule limit_reads:
		output: 'reads/limited-{}.{{nr}}.fastq'.format(config.limit)
		input: 'reads.{nr}.fastq.gz'
		shell:
			'sqt-fastqmod --limit {config.limit} {input} > {output}'
else:
	rule decompress_reads:
		output: "reads/decompressed.{nr}.fastq"
		input: "reads.{nr}.fastq.gz"
		shell: "zcat {input} > {output}"


if config.merge_program == 'flash':
	rule flash_merge:
		"""Use FLASH to merge paired-end reads"""
		output: MERGED_READS
		input: READS1, READS2
		resources: time=60
		threads: 8
		log: 'reads/flash.log'
		shell:
			# -M: maximal overlap (2x300, 420-450bp expected fragment size)
			"flash -t {threads} -c -M {config.flash_maximum_overlap} {input} 2> >(tee {log} >&2) | pigz > {output}"
elif config.merge_program == 'pear':
	rule pear_merge:
		"""Use pear to merge paired-end reads"""
		output:
			'reads/pear.unassembled.forward.fastq',
			'reads/pear.unassembled.reverse.fastq',
			'reads/pear.discarded.fastq',
			fastq=MERGED_READS
		input: READS1, READS2
		resources: time=60
		threads: 8
		log: 'reads/pear.log'
		shell:
			r"""
			pear -j {threads} -f {input[0]} -r {input[1]} -o reads/pear | tee {log} && \
			gzip < reads/pear.assembled.fastq > {output.fastq} && rm reads/pear.assembled.fastq
			"""
else:
	sys.exit("merge_program {config.merge_program!r} given in configuration file not recognized".format(config=config))


rule merged_read_length_histogram:
	output:
		txt="stats/merged.readlengths.txt",
		pdf="stats/merged.readlengths.pdf"
	input:
		fastq=MERGED_READS
	shell:
		"""sqt-readlenhisto --bins 100 --left 300 --title "Lengths of merged reads" --plot {output.pdf} {input}  > {output.txt}"""


rule unique_read_length_histogram:
	output:
		txt="stats/unique.readlengths.txt",
		pdf="stats/unique.readlengths.pdf"
	input:
		fastq=UNIQUE_READS
	shell:
		"""sqt-readlenhisto --bins 100 --left 300 --title "Lengths of pre-processed reads" --plot {output.pdf} {input}  > {output.txt}"""


rule barcode_stats:
	"""Print out number of random barcodes in the library
	TODO
	- run this only if random barcodes are actually used
	- make sure that a stranded protocol is used
	"""
	output: txt="stats/barcodes.txt"
	input: fastq=MERGED_READS
	shell:
		"""
		zcat {input} | awk 'NR%4==2 {{print substr($1,1,12)}}' | grep -v N | sort -u | wc -l > {output}
		"""


rule stats_numbers:
	output: txt="{dir}/counts.txt"
	input:
		reads=READS1,
		merged=MERGED_READS,
		unique=UNIQUE_READS,
		unique_table="{dir}/unique.assigned.tab",
		consensus_table="{dir}/consensus.assigned.tab",
	shell:
		"""
		echo -n "Number of paired-end reads: " > {output}
		awk 'END {{ print NR/4 }}' {input.reads} >> {output}
		echo -n "Number of barcodes (looking at 1st read in pair): " >> {output}
		awk 'NR % 4 == 2 {{ print substr($1, 1, 12) }}' {input.reads} | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of merged sequences: " >> {output}
		zcat {input.merged} | awk 'END {{ print NR/4 }}' >> {output}
		echo -n "Number of barcodes in merged sequences: " >> {output}
		zcat {input.merged} | awk 'NR % 4 == 2 {{ print substr($1, 1, 12) }}' | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of unique sequences: " >> {output}
		grep -c '^>' {input.unique} >> {output}
		echo -n "Number of barcodes in unique sequences: " >> {output}
		grep -A 1 '^>' {input.unique} | awk '!/^>/ && $1 != "--" {{ print substr($1,1,12) }}' | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of barcodes in unique table: " >> {output}
		cut -f35 {input.unique_table} | sed 1d | awk '{{print substr($1, 1, 12) }}' | sort -u | grep -v N | wc -l >>  {output}
		echo -n "Number of sequences in final consensus table: " >> {output}
		sed 1d {input.consensus_table} | wc -l >> {output}
		"""


# Remove primer sequences

if config.forward_primers:
	# At least one forward primer is to be removed
	rule trim_forward_primers:
		output: fastq='reads/forward-primer-trimmed.fastq.gz'
		input: fastq=MERGED_READS
		resources: time=120
		log: 'reads/forward-primer-trimmed.cutadapt.log'
		run:
			primers = config.forward_primers
			param = ' '.join('-g ^{}'.format(seq) for seq in primers)
			if not config.stranded:
				param += ' ' + ' '.join('-a {}$'.format(reverse_complement(seq)) for seq in config.forward_primers)
			shell(
			"""
			cutadapt --discard-untrimmed {param} -o {output.fastq} {input.fastq} | tee {log}
			""")
else:
	# No trimming, just symlink the file
	rule dont_trim_forward_primers:
		output: fastq='reads/forward-primer-trimmed.fastq.gz'
		input: fastq=MERGED_READS
		resources: time=1
		run:
			try:
				os.remove(output.fastq)
			except FileNotFoundError:
				pass
			relative_symlink(input.fastq, output.fastq)


if config.reverse_primers:
	# At least one reverse primer is to be removed
	rule trim_reverse_primers:
		output: fastq='reads/trimmed.fastq.gz'
		input: fastq='reads/forward-primer-trimmed.fastq.gz'
		resources: time=120
		log: 'reads/trimmed.cutadapt.log'
		run:
			primers = config.reverse_primers
			# Reverse primers should appear reverse-complemented at the 3' end
			# of the merged read.
			param = ' '.join('-a {}$'.format(reverse_complement(seq)) for seq in primers)
			if not config.stranded:
				param += ' ' + ' '.join('-g ^{}'.format(seq) for seq in config.reverse_primers)
			shell(
			"""
			cutadapt --discard-untrimmed {param} -o {output.fastq} {input.fastq} | tee {log}
			""")
else:
	# No trimming, just symlink the file
	rule dont_trim_reverse_primers:
		output: fastq='reads/trimmed.fastq.gz'
		input: fastq='reads/forward-primer-trimmed.fastq.gz'
		resources: time=1
		run:
			try:
				os.remove(output.fastq)
			except FileNotFoundError:
				pass
			relative_symlink(input.fastq, output.fastq)


rule fastqc:
	output:
		zip='fastqc/{file}.zip',
		png='fastqc/{file}/Images/per_base_quality.png',
		html='fastqc/{file}_fastqc.html'
	input: fastq='{file}.fastq.gz'
	shell:
		r"""
		rm -rf fastqc/{wildcards.file}/ fastqc/{wildcards.file}_fastqc/ && \
		fastqc -o fastqc {input} && \
		mv fastqc/{wildcards.file}_fastqc.zip {output.zip} && \
		unzip -o -d fastqc/ {output.zip} && \
		mv fastqc/{wildcards.file}_fastqc/ fastqc/{wildcards.file}
		"""


rule fastq_to_fasta:
	"""
	* Convert from FASTQ to FASTA
	* Remove low-quality sequences
	* Discard too short sequences
	"""
	output: fasta="reads/filtered.fasta"
	input: fastq="reads/trimmed.fastq.gz"
	params: max_errors="--max-errors {config.maximum_expected_errors}" if config.maximum_expected_errors is not None else ""
	shell:
		"sqt-fastqmod {params.max_errors} --minimum-length {config.minimum_merged_read_length} --fasta {input.fastq} > {output.fasta}"


rule dereplicate:
	"""Collapse identical sequences with VSEARCH"""
	output: fasta=UNIQUE_READS
	input: fasta="reads/filtered.fasta"
	shell:
		'vsearch --derep_fulllength {input.fasta} --strand both --output {output.fasta} --sizeout'


# TODO this rule is unused
rule cluster:
	"""
	TODO Daniel ran this three times (at 99%, 98% and 97% identity) in order to
	avoid a specific type of misclustering.
	"""
	output:
		fasta="clustered.fasta",  # centroids
		uc="clustered.uc"
	input: fasta=UNIQUE_READS
	resources: time=36*60, mem=32000
	threads: 4
	shell:
		# TODO -idprefix 5?
		r"""
		{config.cluster_program} -threads {threads} -cluster_fast {input.fasta} -id 0.97 -uc {output.uc} \
			-idprefix 5 -sizeout --centroids {output.fasta}
		"""


rule copy_dj_database:
	"""Copy D and J gene database into the iteration folder"""
	output:
		fasta="{base}/database/{species}_{gene,[DJ]}.fasta"
	input:
		fasta="database/{species}_{gene}.fasta"
	shell:
		"cp -p {input} {output}"


rule v_database_iteration_1:
	"""Copy original V gene database into the iteration 1 folder"""
	output:
		fasta="iteration-01/database/{species}_V.fasta"
	input:
		fasta="database/{species}_V.fasta"
	shell:
		"cp -p {input} {output}"


for i in range(2, config.iterations + 1):
	rule:
		output:
			fasta='iteration-{nr:02d}/database/{{species}}_V.fasta'.format(nr=i)
		input:
			fasta='iteration-{nr:02d}/new_V_database.fasta'.format(nr=i-1)
		shell:
			"cp -p {input.fasta} {output.fasta}"

if config.iterations == 0:
	# Copy over the input database (would be nice to avoid this)
	rule copy_database:
		output:
			fasta='final/database/{species}_V.fasta'
		input:
			fasta='database/{species}_V.fasta'
		shell:
			"cp -p {input.fasta} {output.fasta}"
else:
	rule copy_final_v_database:
		output:
			fasta='final/database/{species}_V.fasta'.format(species=config.species)
		input:
			fasta='iteration-{nr:02d}/new_V_database.fasta'.format(nr=config.iterations)
		shell:
			"cp -p {input.fasta} {output.fasta}"


rule makeblastdb:
	output: "{dir}/{species}_{gene,[VDJ]}.nhr"  # and nin nog nsd nsi nsq
	input: fasta="{dir}/{species}_{gene}.fasta"
	params: dbname="{dir}/{species}_{gene}"
	log: '{dir}/{species}_{gene}.log'
	threads: 100  # force to run as single job
	run:
		with FastaReader(input.fasta) as fr:
			sequences = list(fr)
		if not sequences:
			raise ValueError("The FASTA file {} is empty, cannot continue!".format(input.fasta))
		shell(r"""
		makeblastdb -parse_seqids -dbtype nucl -in {input.fasta} -out {params.dbname} >& {log}
		grep 'Error: ' {log} && {{ echo "makeblastdb failed when creating {params.dbname}"; false; }} || true
		""")


rule symlink_iteration_reads:
	output:
		fasta='{dir}/unique.fasta'
	input:
		fasta='reads/unique.fasta'
	shell:
		# Need to use a hard link, otherwise snakemake complains about existing
		# output file if workflow is re-run
		'ln -f {input.fasta} {output.fasta}'


rule igypipe_igblast:
	output:
		txtgz="{dir}/{base}.igblast.txt.gz"
	input:
		fasta='{dir}/{base}.fasta',
		db_v="{{dir}}/database/{species}_V.nhr".format(species=config.species),
		db_d="{{dir}}/database/{species}_D.nhr".format(species=config.species),
		db_j="{{dir}}/database/{species}_J.nhr".format(species=config.species)
	params:
		penalty='--penalty {}'.format(config.mismatch_penalty) if config.mismatch_penalty is not None else '',
		database='{dir}/database'
	threads: 16
	shell:
		"igypipe igblast --threads {threads} {params.penalty} "
		"--species {config.species} {params.database} {input.fasta} | "
		"gzip > {output.txtgz}.tmp && "
		"mv {output.txtgz}.tmp {output.txtgz}"


rule igypipe_parse:
	output:
		tab="{dir}/{base}.assigned.tab",
		#h5="{dir}/{base}.assigned.h5"
	input:
		txt="{dir}/{base}.igblast.txt.gz",
		fasta="{dir}/{base}.fasta"
	shell:
		"igypipe parse --barcode-length {config.barcode_length} --rename {config.library_name!r}_ {input.txt} {input.fasta} > {output.tab}"


rule igypipe_filter:
	output:
		filtered="{dir}/{base}.filtered.tab"
	input:
		assigned="{dir}/{base}.assigned.tab"
	shell:
		"igypipe filter {input} > {output}"


rule igypipe_group:
	"""Group by barcode"""
	output:
		pdf="{base}/stats/groupsizes.pdf",
		tab="{base}/groups.tab",
		fasta="{base}/consensus.fasta"
	input:
		tab="{base}/unique.filtered.tab"
	log: "{base}/unique.consensus.log"
	shell:
		"igypipe group --program={config.multialign_program} --plot-sizes {output.pdf} --groups-output {output.tab} {input.tab} > {output.fasta} 2> {log}"


rule igypipe_count:
	output:
		plot="{dir}/{base}.{gene,[VDJ]}_usage.pdf",
		counts="{dir}/{base}.{gene}_usage.tab"
	input:
		reference="{{dir}}/database/{species}_{{gene}}.fasta".format(species=config.species),
		tab="{dir}/{base}.filtered.tab"
	shell:
		"igypipe count --database {input.reference} --gene {wildcards.gene} "
		"{input.tab} {output.plot} > {output.counts}.tmp && "
		"mv {output.counts}.tmp {output.counts}"


rule igypipe_clusterplot:
	output:
		done="{dir}/clusterplots/done"
	input:
		tab="{dir}/unique.filtered.tab"
	params:
		clusterplots="{dir}/clusterplots/",
		ignore_j='--ignore-J' if config.ignore_j else ''
	shell:
		"igypipe clusterplot {params.ignore_j} {input.tab} {params.clusterplots} && touch {output.done}"


rule igypipe_discover:
	"""Discover potential new V gene sequences"""
	output:
		tab="{dir}/candidates.tab"
	input:
		v_reference="{{dir}}/database/{species}_V.fasta".format(species=config.species),
		tab="{dir}/unique.filtered.tab"
	params:
		ignore_j='--ignore-J' if config.ignore_j else ''
	threads: 4  # TODO this could be increased up to 6
	shell:
		"igypipe discover -j {threads} --cluster --subsample {config.subsample} "
		"--window-width 2 --database {input.v_reference} {params.ignore_j} "
		"{input.tab} > "
		"{output.tab}.tmp && mv {output.tab}.tmp {output.tab}"


rule igypipe_compose:
	"""Construct a new database out of the discovered sequences"""
	output:
		fasta='iteration-{nr}/new_V_database.fasta'
	input:
		tab='iteration-{nr}/candidates.tab'
	run:
		nr = int(wildcards.nr, base=10)
		criteria = '--looks-like-V --max-differences=2 '
		if nr == config.iterations:
			# Apply stricter filtering criteria for final iteration
			criteria += '--unique-CDR3=3 --consensus-seqs=200'
		else:
			criteria += '--unique-CDR3=2'
		shell("igypipe compose {criteria} {input.tab} > {output.fasta}.tmp && mv {output.fasta}.tmp {output.fasta}")


rule stats_correlation_V_J:
	output:
		pdf="{base}.correlationVJ.pdf"
	input:
		table="{base}.assigned.tab"
	run:
		import matplotlib
		matplotlib.use('pdf')
		# sns.heatmap will not work properly with the object-oriented interface,
		# so use pyplot
		import matplotlib.pyplot as plt
		import seaborn as sns
		import numpy as np
		import pandas as pd
		from collections import Counter
		table = igypipe.read_table(input.table)
		fig = plt.figure(figsize=(29.7/2.54, 21/2.54))
		counts = np.zeros((21, 11), dtype=np.int64)
		counter = Counter(zip(table['V_errors'], table['J_errors']))
		for (v,j), count in counter.items():
			if v is not None and v < counts.shape[0] and j is not None and j < counts.shape[1]:
				counts[v,j] = count
		df = pd.DataFrame(counts.T)[::-1]
		df.index.name = 'J errors'
		df.columns.name = 'V errors'
		sns.heatmap(df, annot=True, fmt=',d', cbar=False)
		fig.suptitle('V errors vs. J errors in unfiltered sequences')
		fig.set_tight_layout(True)
		fig.savefig(output.pdf)


rule plot_errorhistograms:
	output:
		pdf='{base}.errorhistograms.pdf',
	input:
		table='{base}.filtered.tab'
	params:
		ignore_j='--ignore-J' if config.ignore_j else ''
	shell:
		'igypipe errorplot {params.ignore_j} {input.table} {output.pdf}'


rule dendrogram:
	output:
		pdf='{dir}/{gene}_dendrogram.pdf'
	input:
		fasta='{{dir}}/database/{species}_{{gene}}.fasta'.format(species=config.species)
	shell:
		'igypipe dendrogram --mark database/{config.species}_{wildcards.gene}.fasta {input.fasta} {output.pdf}'
