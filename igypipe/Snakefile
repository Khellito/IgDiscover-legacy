# kate: syntax Python;
"""
Required modules:

module use /proj/b2014342/sw/modules
module load igypipe

These are the main files that the pipeline creates, in the order in which they
are created:

reads/merged.fastq.gz -- merged reads
reads/trimmed.fastq.gz -- primers removed from merged reads
reads/filtered.fasta  -- too short sequences removed, converted to FASTA
reads/collapsed.fasta -- collapsed sequences (duplicates removed)
unique.igblast.txt.gz -- IgBLAST output
unique.assigned.tab -- parsed IgBLAST output as a tab-delimited table
unique.filtered.tab -- filtered version of the above
groups.tab -- sequences grouped by barcode
consensus.fasta -- contains one consensus sequence for each group
consensus.igblast.txt -- consensus sequences sent through IgBLAST
consensus.assigned.tab -- parsed IgBLAST output
"""
from matplotlib.figure import Figure
from matplotlib.backends.backend_pdf import FigureCanvasPdf
import seaborn as sns
from sqt.dna import reverse_complement
import igypipe

class Config:
	def __init__(self, path):
		# Set some defaults.
		self.merge_program = 'pear'
		self.limit = None  # or an integer
		self.cluster_program = 'vsearch'
		self.multialign_program = 'muscle-fast'
		self.maximum_expected_errors = None  # or an integer
		self.minimum_merged_read_length = 300
		self.trim_primers = False
		self.mismatch_penalty = None
		self.barcode_length = 0
		self.iterations = 1

		self.read_from(path)

	def read_from(self, path):
		try:
			exec(open(path).read(), self.__dict__)
		except FileNotFoundError:
			sys.exit("Pipeline configuration file {!r} not found. Please create it!".format(path))
		# Convert all-uppercase attributes to lowercase
		for attribute, value in list(vars(self).items()):
			if attribute == attribute.upper():
				setattr(self, attribute.lower(), value)
		if self.trim_primers:
			if not self.reverse_primers:
				sys.exit("The list of reverse_primers is empty. This will not work correctly.")
			if not self.forward_primers:
				sys.exit("The list of forward_primers is empty. This will not work correctly.")


config = Config('pipeline.conf')

# This command is run before every shell command and helps to catch errors early
shell.prefix("set -euo pipefail;")

if config.limit:
	READS1 = 'reads/limited-{}.1.fastq'.format(config.limit)
	READS2 = 'reads/limited-{}.2.fastq'.format(config.limit)
else:
	READS1 = 'reads/decompressed.1.fastq'
	READS2 = 'reads/decompressed.2.fastq'


MERGED_READS = 'reads/merged.fastq.gz'
UNIQUE_READS = 'reads/unique.fasta'


PER_ITERATION_TARGETS = [
	'discover.tab',
	'new_V_database.fasta',
	'clusterplots/done',
	'counts.txt',
	'unique.correlationVJ.pdf',
	'unique.errorhistograms.pdf',
	'unique.v_usage.tab',
	'unique.v_usage.pdf',
]
# targets for last iteration only
FINAL_TARGETS = [
	'consensus.correlationVJ.pdf',
	'consensus.v_usage.tab',
	'consensus.v_usage.pdf',
]
TARGETS = expand('iteration-{nr:03d}/{path}', nr=range(1, config.iterations+1), path=PER_ITERATION_TARGETS)
TARGETS += expand('iteration-{nr:03d}/{{path}}'.format(nr=config.iterations), path=FINAL_TARGETS)
TARGETS += ['stats/barcodes.txt']


rule all:
	input:
		TARGETS


if config.limit:
	rule limit_reads:
		output: 'reads/limited-{}.{{nr}}.fastq'.format(config.limit)
		input: 'reads.{nr}.fastq.gz'
		shell:
			'sqt-fastqmod --limit {config.limit} {input} > {output}'
else:
	rule decompress_reads:
		output: "reads/decompressed.{nr}.fastq"
		input: "reads.{nr}.fastq.gz"
		shell: "zcat {input} > {output}"


if config.merge_program == 'flash':
	rule flash_merge:
		"""Use FLASH to merge paired-end reads"""
		output: MERGED_READS
		input: READS1, READS2
		resources: time=60
		threads: 8
		log: 'reads/flash.log'
		shell:
			# -M: maximal overlap (2x300, 420-450bp expected fragment size)
			"flash -t {threads} -c -M {config.flash_maximum_overlap} {input} 2> >(tee {log} >&2) | pigz > {output}"
elif config.merge_program == 'pear':
	rule pear_merge:
		"""Use pear to merge paired-end reads"""
		output:
			'reads/pear.unassembled.forward.fastq',
			'reads/pear.unassembled.reverse.fastq',
			'reads/pear.discarded.fastq',
			fastq=MERGED_READS
		input: READS1, READS2
		resources: time=60
		threads: 8
		shell:
			r"""
			pear -j {threads} -f {input[0]} -r {input[1]} -o reads/pear && \
			gzip < reads/pear.assembled.fastq > {output.fastq} && rm reads/pear.assembled.fastq
			"""
else:
	sys.exit("merge_program given in configuration file not recognized")


rule read_length_histogram:
	# TODO on which data should this be created?
	output:
		txt="stats/readlengthhisto.txt",
		pdf="stats/readlengthhisto.pdf"
	input:
		fastq=MERGED_READS
	shell:
		"sqt-readlenhisto --plot {output.pdf} {input}  > {output.txt}"


rule barcode_stats:
	"""Print out number of random barcodes in the library
	TODO
	- run this only if random barcodes are actually used
	- make sure that a stranded protocol is used
	"""
	output: txt="stats/barcodes.txt"
	input: fastq=MERGED_READS
	shell:
		"""
		zcat {input} | awk 'NR%4==2 {{print substr($1,1,12)}}' | grep -v N | sort -u | wc -l > {output}
		"""


rule stats_numbers:
	output: txt="{dir}/counts.txt"
	input:
		reads=READS1,
		merged=MERGED_READS,
		unique=UNIQUE_READS,
		unique_table="{dir}/unique.assigned.tab",
		consensus_table="{dir}/consensus.assigned.tab",
	shell:
		"""
		echo -n "Number of paired-end reads: " > {output}
		awk 'END {{ print NR/4 }}' {input.reads} >> {output}
		echo -n "Number of barcodes (looking at 1st read in pair): " >> {output}
		awk 'NR % 4 == 2 {{ print substr($1, 1, 12) }}' {input.reads} | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of merged sequences: " >> {output}
		zcat {input.merged} | awk 'END {{ print NR/4 }}' >> {output}
		echo -n "Number of barcodes in merged sequences: " >> {output}
		zcat {input.merged} | awk 'NR % 4 == 2 {{ print substr($1, 1, 12) }}' | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of unique sequences: " >> {output}
		grep -c '^>' {input.unique} >> {output}
		echo -n "Number of barcodes in unique sequences: " >> {output}
		grep -A 1 '^>' {input.unique} | awk '!/^>/ && $1 != "--" {{ print substr($1,1,12) }}' | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of barcodes in unique table: " >> {output}
		cut -f35 {input.unique_table} | sed 1d | awk '{{print substr($1, 1, 12) }}' | sort -u | grep -v N | wc -l >>  {output}
		echo -n "Number of sequences in final consensus table: " >> {output}
		sed 1d {input.consensus_table} | wc -l >> {output}
		"""


rule stats_correlation_V_J:
	output:
		pdf="{base}.correlationVJ.pdf"
	input:
		table="{base}.assigned.tab"
	run:
		table = igypipe.read_table(input.table)
		fig = Figure(figsize=(30/2.54, 21/2.54))
		ax = fig.gca()
		ax.set_xlabel('V%SHM')
		ax.set_ylabel('J%SHM')
		if len(table) > 0:
			ax.scatter(table['V_SHM'], table['J_SHM'])
		ax.set_xlim(left=-0.1)
		ax.set_ylim(bottom=-0.1)
		ax.set_title('Correlation between V%SHM and J%SHM')
		FigureCanvasPdf(fig).print_figure(output.pdf)


rule stats_plot_errorhistograms:
	output:
		pdf='{base}.errorhistograms.pdf',
	input:
		table='{base}.filtered.tab'
	shell:
		'igypipe errorplot {input.table} {output.pdf}'


# Adjust the primer sequences so they are correctly reverse-complemented.
# When a forward primer fwd and a reverse primer rev are given, then we need to
# search for:
# * fwd in the beginning, revcomp(rev) in the end
# * rev in the beginning, revcomp(fwd) in the end
PRIMERS = [
	config.forward_primers[:], [reverse_complement(seq) for seq in config.reverse_primers]
]
PRIMERS[0].extend(config.reverse_primers)
PRIMERS[1].extend(reverse_complement(seq) for seq in config.forward_primers)

rule trim_primers_fivep:
	output: fastq="reads/trimmed-fivep.fastq.gz"
	input: fastq=MERGED_READS
	resources: time=60
	log: 'reads/cutadapt-fiveprime.log'
	params:
		five_p=" ".join("-g ^{}".format(seq) for seq in PRIMERS[0])
	shell:
		r"""
		cutadapt --discard-untrimmed {params.five_p} -o {output.fastq} {input.fastq} | tee {log}
		"""


rule trim_primers_threep:
	output: fastq="reads/trimmed.fastq.gz"
	input: fastq="reads/trimmed-fivep.fastq.gz"
	resources: time=60
	log: 'reads/cutadapt-threeprime.log'
	params:
		three_p=" ".join("-a {}$".format(seq) for seq in PRIMERS[1])
	shell:
		r"""
		cutadapt --discard-untrimmed {params.three_p} -o {output.fastq} {input.fastq} | tee {log}
		"""


rule fastqc:
	output:
		zip='fastqc/{file}.zip',
		png='fastqc/{file}/Images/per_base_quality.png',
		html='fastqc/{file}_fastqc.html'
	input: fastq='{file}.fastq'
	shell:
		r"""
		rm -rf fastqc/{wildcards.file}/ fastqc/{wildcards.file}_fastqc/ && \
		fastqc -o fastqc {input} && \
		mv fastqc/{wildcards.file}_fastqc.zip {output.zip} && \
		unzip -o -d fastqc/ {output.zip} && \
		mv fastqc/{wildcards.file}_fastqc/ fastqc/{wildcards.file}
		"""


rule fastq_to_fasta:
	"""
	* Convert from FASTQ to FASTA
	* Remove low-quality sequences
	* Discard too short sequences
	"""
	output: fasta="reads/filtered.fasta"
	input: fastq="reads/trimmed.fastq.gz" if config.trim_primers else MERGED_READS
	params: max_errors="--max-errors {config.maximum_expected_errors}" if config.maximum_expected_errors is not None else ""
	shell:
		"sqt-fastqmod {params.max_errors} --minimum-length {config.minimum_merged_read_length} --fasta {input.fastq} > {output.fasta}"


rule dereplicate:
	"""Collapse identical sequences with VSEARCH"""
	output: fasta=UNIQUE_READS
	input: fasta="reads/filtered.fasta"
	shell:
		"""vsearch --derep_fulllength {input.fasta} --strand both --output {output.fasta} --sizeout"""


# TODO this rule is unused
rule cluster:
	"""
	TODO Daniel ran this three times (at 99%, 98% and 97% identity) in order to
	avoid a specific type of misclustering.
	"""
	output:
		fasta="clustered.fasta",  # centroids
		uc="clustered.uc"
	input: fasta=UNIQUE_READS
	resources: time=36*60, mem=32000
	threads: 4
	shell:
		# TODO -idprefix 5?
		r"""
		{config.cluster_program} -threads {threads} -cluster_fast {input.fasta} -id 0.97 -uc {output.uc} \
			-idprefix 5 -sizeout --centroids {output.fasta}
		"""


rule copy_dj_database:
	"""Copy D and J gene database into the iteration folder"""
	output:
		fasta="iteration-{nr}/database/{species}_{gene,[DJ]}.fasta"
	input:
		fasta="database/{species}_{gene}.fasta"
	shell:
		"cp -p {input} {output}"


rule v_database_iteration_1:
	"""Copy original V gene database into the iteration 1 folder"""
	output:
		fasta="iteration-001/database/{species}_V.fasta"
	input:
		fasta="database/{species}_V.fasta"
	shell:
		"cp -p {input} {output}"


for i in range(2, config.iterations + 1):
	rule:
		output:
			fasta='iteration-{nr:03d}/database/{{species}}_V.fasta'.format(nr=i)
		input:
			fasta='iteration-{nr:03d}/new_V_database.fasta'.format(nr=i-1)
		shell:
			"cp -p {input.fasta} {output.fasta}"


rule makeblastdb:
	input: fasta="{dir}/{species}_{gene}.fasta"
	output: "{dir}/{species}_{gene,[VDJ]}.nhr"  # and nin nog nsd nsi nsq
	params: dbname="{dir}/{species}_{gene}"
	log: '{dir}/{species}_{gene}.log'
	threads: 100  # force to run as single job
	shell:
		r"""
		makeblastdb -parse_seqids -dbtype nucl -in {input.fasta} -out {params.dbname} >& {log}
		grep 'Error: ' {log} && {{ echo "makeblastdb failed when creating {params.dbname}"; false; }} || true
		"""


rule symlink_iteration_reads:
	output:
		fasta='{dir}/unique.fasta'
	input:
		fasta='reads/unique.fasta'
	shell:
		'ln -s ../{input.fasta} {output.fasta}'


rule igypipe_igblast:
	output:
		txtgz="{dir}/{base}.igblast.txt.gz"
	input:
		fasta='{dir}/{base}.fasta',
		db_v="{{dir}}/database/{species}_V.nhr".format(species=config.species),
		db_d="{{dir}}/database/{species}_D.nhr".format(species=config.species),
		db_j="{{dir}}/database/{species}_J.nhr".format(species=config.species)
	params:
		penalty='--penalty {}'.format(config.mismatch_penalty) if config.mismatch_penalty is not None else '',
		database='{dir}/database'
	threads: 16
	shell:
		r"""
		igypipe igblast --threads {threads} {params.penalty} --species {config.species} {params.database} {input.fasta} | gzip > {output.txtgz}
		"""


rule igypipe_parse:
	output:
		tab="{dir}/{base}.assigned.tab"
	input:
		txt="{dir}/{base}.igblast.txt.gz",
		fasta="{dir}/{base}.fasta"
	params:
		dirname=os.path.basename(os.getcwd()) + '_'
	shell:
		"igypipe parse --barcode-length {config.barcode_length} --rename {params.dirname!r} {input.txt} {input.fasta} > {output.tab}"


rule igypipe_filter:
	output:
		filtered="{dir}/{base}.filtered.tab"
	input:
		assigned="{dir}/{base}.assigned.tab"
	shell:
		"igypipe filter {input} > {output}"


rule igypipe_group:
	"""Group by barcode"""
	output:
		pdf="{base}/stats/groupsizes.pdf",
		tab="{base}/groups.tab",
		fasta="{base}/consensus.fasta"
	input:
		tab="{base}/unique.filtered.tab"
	log: "{base}/unique.consensus.log"
	shell:
		"igypipe group --program={config.multialign_program} --plot-sizes {output.pdf} --groups-output {output.tab} {input.tab} > {output.fasta} 2> {log}"


rule igypipe_count:
	output:
		plot="{dir}/{base}.v_usage.pdf",
		counts="{dir}/{base}.v_usage.tab"
	input:
		v_reference="{{dir}}/database/{species}_V.fasta".format(species=config.species),
		tab="{dir}/{base}.filtered.tab"
	shell:
		"igypipe count --reference {input.v_reference} {input.tab} {output.plot} > {output.counts}"


rule igypipe_clusterplot:
	output:
		done="{dir}/clusterplots/done"
	input:
		tab="{dir}/unique.filtered.tab"
	params:
		clusterplots="{dir}/clusterplots/"
	shell:
		"igypipe clusterplot {input.tab} {params.clusterplots} && touch {output.done}"


rule igypipe_singledisco:
	"""Discover potential new V gene sequences"""
	output:
		tab="{dir}/discover.tab"
	input:
		v_reference="{{dir}}/database/{species}_V.fasta".format(species=config.species),
		tab="{dir}/unique.filtered.tab"
	threads: 4
	shell:
		"igypipe singledisco -j {threads} --cluster --window-width 2 --database {input.v_reference} {input.tab} > {output.tab}"


rule igypipe_compose:
	"""Construct a new database out of the discovered sequences"""
	output:
		fasta='{dir}/new_V_database.fasta'
	input:
		database="{{dir}}/database/{species}_V.fasta".format(species=config.species),
		tab='{dir}/discover.tab'
	shell:
		"igypipe compose --unique-CDR3 2 --database {input.database} {input.tab} > {output.fasta}"
