# kate: syntax Python;
"""
"""
import yaml
from sqt.dna import reverse_complement
from sqt import FastaReader
import igdiscover
from igdiscover.utils import relative_symlink

class Config:
	def __init__(self, path):
		# Set some defaults.
		self.merge_program = 'pear'
		self.flash_maximum_overlap = 300
		self.limit = None  # or an integer
		self.cluster_program = 'vsearch'
		self.multialign_program = 'muscle-fast'
		self.maximum_expected_errors = None  # or an integer
		self.minimum_merged_read_length = 300
		self.mismatch_penalty = None
		self.barcode_length = 0
		self.iterations = 1
		self.ignore_j = False
		self.subsample = 500
		self.stranded = False
		self.forward_primers = None
		self.reverse_primers = None
		self.library_name = os.path.basename(os.getcwd())
		self.read_from(path)

	def read_from(self, path):
		with open(path) as f:
			content = f.read()
		self.__dict__.update(yaml.safe_load(content))

try:
	config = Config('igdiscover.yaml')
except FileNotFoundError:
	sys.exit("Pipeline configuration file 'igdiscover.yaml' not found. Please create it!")

print('IgDiscover configuration:')
for k, v in sorted(vars(config).items()):
	# TODO the following line is only necessary for non-YAML configurations
	if k.startswith('_'):
		continue
	print('   ', k, ': ', repr(v), sep='')

# This command is run before every shell command and helps to catch errors early
shell.prefix("set -euo pipefail;")

if config.limit:
	READS1 = 'reads/limited-{}.1.fastq.gz'.format(config.limit)
	READS2 = 'reads/limited-{}.2.fastq.gz'.format(config.limit)
else:
	READS1 = 'reads.1.fastq.gz'
	READS2 = 'reads.2.fastq.gz'

MERGED_READS = 'reads/merged.fastq.gz'
UNIQUE_READS = 'reads/unique.fasta.gz'

# Targets for each iteration
ITERATION_TARGETS = [
	'clusterplots/done',
	'counts.txt',
	#'unique.correlationVJ.pdf',
	'unique.errorhistograms.pdf',
	'unique.V_usage.tab',
	'unique.V_usage.pdf',
	'V_dendrogram.pdf',
]
# Targets for non-final iterations
DISCOVERY_TARGETS = [
	'candidates.tab',
	'new_V_database.fasta',
]
# Targets for final iteration
FINAL_TARGETS = [
	#'consensus.correlationVJ.pdf',
	'consensus.V_usage.tab',
	'consensus.V_usage.pdf',
]
TARGETS = expand('iteration-{nr:02d}/{path}', nr=range(1, config.iterations+1), path=ITERATION_TARGETS+DISCOVERY_TARGETS)
TARGETS += expand('final/{path}', path=ITERATION_TARGETS+FINAL_TARGETS)
TARGETS += ['stats/barcodes.txt', 'stats/unique.readlengths.pdf', 'stats/merged.readlengths.pdf']

# Add also intermediate files
TARGETS += [READS1, READS2, MERGED_READS, UNIQUE_READS]

rule all:
	input:
		TARGETS


if config.limit:
	rule limit_reads:
		output: 'reads/limited-{}.{{nr}}.fastq.gz'.format(config.limit)
		input: 'reads.{nr}.fastq.gz'
		shell:
			'sqt fastxmod --limit {config.limit} {input} | gzip > {output}'


if config.merge_program == 'flash':
	rule flash_merge:
		"""Use FLASH to merge paired-end reads"""
		output: MERGED_READS
		input: READS1, READS2
		resources: time=60
		threads: 8
		log: 'reads/flash.log'
		shell:
			# -M: maximal overlap (2x300, 420-450bp expected fragment size)
			"flash -t {threads} -c -M {config.flash_maximum_overlap} {input} 2> >(tee {log} >&2) | gzip > {output}"
elif config.merge_program == 'pear':
	rule pear_merge:
		"""Use pear to merge paired-end reads"""
		output:
			unmerged1='reads/pear.unassembled.forward.fastq.gz',
			unmerged2='reads/pear.unassembled.reverse.fastq.gz',
			discarded='reads/pear.discarded.fastq.gz',
			fastq=MERGED_READS,
		input: READS1, READS2
		resources: time=60
		threads: 8
		log: 'reads/pear.log'
		shell:
			r"""
			pear -j {threads} -f {input[0]} -r {input[1]} -o reads/pear | tee {log}
			for f in reads/pear.{{unassembled.forward,unassembled.reverse,assembled,discarded}}.fastq; do
				gzip -f $f
			done
			mv reads/pear.assembled.fastq.gz {output.fastq}
			"""
else:
	sys.exit("merge_program {config.merge_program!r} given in configuration file not recognized".format(config=config))


rule merged_read_length_histogram:
	output:
		txt="stats/merged.readlengths.txt",
		pdf="stats/merged.readlengths.pdf"
	input:
		fastq=MERGED_READS
	shell:
		"""sqt readlenhisto --bins 100 --left 300 --title "Lengths of merged reads" --plot {output.pdf} {input}  > {output.txt}"""


rule unique_read_length_histogram:
	output:
		txt="stats/unique.readlengths.txt",
		pdf="stats/unique.readlengths.pdf"
	input:
		fastq=UNIQUE_READS
	shell:
		"""sqt readlenhisto --bins 100 --left 300 --title "Lengths of pre-processed reads" --plot {output.pdf} {input}  > {output.txt}"""


rule barcode_stats:
	"""Print out number of random barcodes in the library
	TODO
	- run this only if random barcodes are actually used
	- make sure that a stranded protocol is used
	"""
	output: txt="stats/barcodes.txt"
	input: fastq=MERGED_READS
	shell:
		"""
		zcat {input} | awk 'NR%4==2 {{print substr($1,1,12)}}' | grep -v N | sort -u | wc -l > {output}
		"""


rule stats_numbers:
	output: txt="{dir}/counts.txt"
	input:
		reads=READS1,
		merged=MERGED_READS,
		unique=UNIQUE_READS,
		unique_table="{dir}/unique.assigned.tab.gz",
		consensus_table="{dir}/consensus.assigned.tab.gz",
	shell:
		"""
		echo -n "Number of paired-end reads: " > {output}
		zcat {input.reads} | awk 'END {{ print NR/4 }}' >> {output}
		echo -n "Number of barcodes (looking at 1st read in pair): " >> {output}
		zcat {input.reads} | awk 'NR % 4 == 2 {{ print substr($1, 1, 12) }}' | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of merged sequences: " >> {output}
		zcat {input.merged} | awk 'END {{ print NR/4 }}' >> {output}
		echo -n "Number of barcodes in merged sequences: " >> {output}
		zcat {input.merged} | awk 'NR % 4 == 2 {{ print substr($1, 1, 12) }}' | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of unique sequences: " >> {output}
		zgrep -c '^>' {input.unique} >> {output}
		echo -n "Number of barcodes in unique sequences: " >> {output}
		zgrep -A 1 '^>' {input.unique} | awk '!/^>/ && $1 != "--" {{ print substr($1,1,12) }}' | sort -u | grep -v N | wc -l >> {output}
		echo -n "Number of barcodes in unique table: " >> {output}
		zcat {input.unique_table} | cut -f35 | sed 1d | awk '{{print substr($1, 1, 12) }}' | sort -u | grep -v N | wc -l >>  {output}
		echo -n "Number of sequences in final consensus table: " >> {output}
		zcat {input.consensus_table} | sed 1d | wc -l >> {output}
		"""


# Remove primer sequences

if config.forward_primers:
	# At least one forward primer is to be removed
	rule trim_forward_primers:
		output: fastq='reads/forward-primer-trimmed.fastq.gz'
		input: fastq=MERGED_READS
		resources: time=120
		log: 'reads/forward-primer-trimmed.cutadapt.log'
		run:
			primers = config.forward_primers
			param = ' '.join('-g ^{}'.format(seq) for seq in primers)
			if not config.stranded:
				param += ' ' + ' '.join('-a {}$'.format(reverse_complement(seq)) for seq in config.forward_primers)
			shell(
			"""
			cutadapt --discard-untrimmed {param} -o {output.fastq} {input.fastq} | tee {log}
			""")
else:
	# No trimming, just symlink the file
	rule dont_trim_forward_primers:
		output: fastq='reads/forward-primer-trimmed.fastq.gz'
		input: fastq=MERGED_READS
		resources: time=1
		run:
			try:
				os.remove(output.fastq)
			except FileNotFoundError:
				pass
			relative_symlink(input.fastq, output.fastq)


if config.reverse_primers:
	# At least one reverse primer is to be removed
	rule trim_reverse_primers:
		output: fastq='reads/trimmed.fastq.gz'
		input: fastq='reads/forward-primer-trimmed.fastq.gz'
		resources: time=120
		log: 'reads/trimmed.cutadapt.log'
		run:
			primers = config.reverse_primers
			# Reverse primers should appear reverse-complemented at the 3' end
			# of the merged read.
			param = ' '.join('-a {}$'.format(reverse_complement(seq)) for seq in primers)
			if not config.stranded:
				param += ' ' + ' '.join('-g ^{}'.format(seq) for seq in config.reverse_primers)
			shell(
			"""
			cutadapt --discard-untrimmed {param} -o {output.fastq} {input.fastq} | tee {log}
			""")
else:
	# No trimming, just symlink the file
	rule dont_trim_reverse_primers:
		output: fastq='reads/trimmed.fastq.gz'
		input: fastq='reads/forward-primer-trimmed.fastq.gz'
		resources: time=1
		run:
			try:
				os.remove(output.fastq)
			except FileNotFoundError:
				pass
			relative_symlink(input.fastq, output.fastq)


rule fastqc:
	output:
		zip='fastqc/{file}.zip',
		png='fastqc/{file}/Images/per_base_quality.png',
		html='fastqc/{file}_fastqc.html'
	input: fastq='{file}.fastq.gz'
	shell:
		r"""
		rm -rf fastqc/{wildcards.file}/ fastqc/{wildcards.file}_fastqc/ && \
		fastqc -o fastqc {input} && \
		mv fastqc/{wildcards.file}_fastqc.zip {output.zip} && \
		unzip -o -d fastqc/ {output.zip} && \
		mv fastqc/{wildcards.file}_fastqc/ fastqc/{wildcards.file}
		"""


rule fastq_to_fasta:
	"""
	* Convert from FASTQ to FASTA
	* Remove low-quality sequences
	* Discard too short sequences
	"""
	output: fasta=temp("reads/filtered.fasta")
	input: fastq="reads/trimmed.fastq.gz"
	params: max_errors="--max-errors {config.maximum_expected_errors}" if config.maximum_expected_errors is not None else ""
	shell:
		"sqt fastxmod {params.max_errors} --minimum-length {config.minimum_merged_read_length} --fasta {input.fastq} > {output.fasta}"


rule dereplicate:
	"""Collapse identical sequences with VSEARCH"""
	output: fastagz=UNIQUE_READS
	input: fasta="reads/filtered.fasta"
	shell:
		'vsearch --derep_fulllength {input.fasta} --strand both  --sizeout --output >(gzip > {output.fastagz})'


rule copy_dj_database:
	"""Copy D and J gene database into the iteration folder"""
	output:
		fasta="{base}/database/{species}_{gene,[DJ]}.fasta"
	input:
		fasta="database/{species}_{gene}.fasta"
	shell:
		"cp -p {input} {output}"


rule v_database_iteration_1:
	"""Copy original V gene database into the iteration 1 folder"""
	output:
		fasta="iteration-01/database/{species}_V.fasta"
	input:
		fasta="database/{species}_V.fasta"
	shell:
		"cp -p {input} {output}"


for i in range(2, config.iterations + 1):
	rule:
		output:
			fasta='iteration-{nr:02d}/database/{{species}}_V.fasta'.format(nr=i)
		input:
			fasta='iteration-{nr:02d}/new_V_database.fasta'.format(nr=i-1)
		shell:
			"cp -p {input.fasta} {output.fasta}"

if config.iterations == 0:
	# Copy over the input database (would be nice to avoid this)
	rule copy_database:
		output:
			fasta='final/database/{species}_V.fasta'
		input:
			fasta='database/{species}_V.fasta'
		shell:
			"cp -p {input.fasta} {output.fasta}"
else:
	rule copy_final_v_database:
		output:
			fasta='final/database/{species}_V.fasta'.format(species=config.species)
		input:
			fasta='iteration-{nr:02d}/new_V_database.fasta'.format(nr=config.iterations)
		shell:
			"cp -p {input.fasta} {output.fasta}"


rule makeblastdb:
	output: "{dir}/{species}_{gene,[VDJ]}.nhr"  # and nin nog nsd nsi nsq
	input: fasta="{dir}/{species}_{gene}.fasta"
	params: dbname="{dir}/{species}_{gene}"
	log: '{dir}/{species}_{gene}.log'
	threads: 100  # force to run as single job
	run:
		with FastaReader(input.fasta) as fr:
			sequences = list(fr)
		if not sequences:
			raise ValueError("The FASTA file {} is empty, cannot continue!".format(input.fasta))
		shell(r"""
		makeblastdb -parse_seqids -dbtype nucl -in {input.fasta} -out {params.dbname} >& {log}
		grep 'Error: ' {log} && {{ echo "makeblastdb failed when creating {params.dbname}"; false; }} || true
		""")


rule symlink_iteration_reads:
	output:
		fasta='{dir}/unique.fasta.gz'
	input:
		fasta='reads/unique.fasta.gz'
	shell:
		# Need to use a hard link, otherwise snakemake complains about existing
		# output file if workflow is re-run
		'ln -f {input.fasta} {output.fasta}'


rule igdiscover_igblast:
	output:
		txtgz="{dir}/{base}.igblast.txt.gz"
	input:
		fasta='{dir}/{base}.fasta.gz',
		db_v="{{dir}}/database/{species}_V.nhr".format(species=config.species),
		db_d="{{dir}}/database/{species}_D.nhr".format(species=config.species),
		db_j="{{dir}}/database/{species}_J.nhr".format(species=config.species)
	params:
		penalty='--penalty {}'.format(config.mismatch_penalty) if config.mismatch_penalty is not None else '',
		database='{dir}/database'
	threads: 16
	shell:
		"igdiscover igblast --threads {threads} {params.penalty} "
		"--species {config.species} {params.database} {input.fasta} | "
		"gzip > {output.txtgz}"


rule igdiscover_parse:
	output:
		tabgz="{dir}/{base}.assigned.tab.gz",
	input:
		txt="{dir}/{base}.igblast.txt.gz",
		fasta="{dir}/{base}.fasta.gz"
	shell:
		"igdiscover parse --barcode-length {config.barcode_length} --rename {config.library_name!r}_ {input.txt} {input.fasta} | gzip > {output.tabgz}"


rule igdiscover_filter:
	output:
		filtered="{dir}/{base}.filtered.tab.gz"
	input:
		assigned="{dir}/{base}.assigned.tab.gz"
	shell:
		"igdiscover filter {input} | gzip > {output}"


rule igdiscover_group:
	"""Group by barcode"""
	output:
		pdf="{base}/stats/groupsizes.pdf",
		tab="{base}/groups.tab",
		fasta="{base}/consensus.fasta.gz"
	input:
		tab="{base}/unique.filtered.tab.gz"
	log: "{base}/unique.consensus.log"
	shell:
		"igdiscover group --program={config.multialign_program} --plot-sizes {output.pdf} --groups-output {output.tab} {input.tab} 2> {log} | gzip > {output.fasta}"


rule igdiscover_count:
	output:
		plot="{dir}/{base}.{gene,[VDJ]}_usage.pdf",
		counts="{dir}/{base}.{gene}_usage.tab"
	input:
		reference="{{dir}}/database/{species}_{{gene}}.fasta".format(species=config.species),
		tab="{dir}/{base}.filtered.tab.gz"
	shell:
		"igdiscover count --database {input.reference} --gene {wildcards.gene} "
		"{input.tab} {output.plot} > {output.counts}"


rule igdiscover_clusterplot:
	output:
		done="{dir}/clusterplots/done"
	input:
		tab="{dir}/unique.filtered.tab.gz"
	params:
		clusterplots="{dir}/clusterplots/",
		ignore_j='--ignore-J' if config.ignore_j else ''
	shell:
		"igdiscover clusterplot {params.ignore_j} {input.tab} {params.clusterplots} && touch {output.done}"


rule igdiscover_discover:
	"""Discover potential new V gene sequences"""
	output:
		tab="{dir}/candidates.tab"
	input:
		v_reference="{{dir}}/database/{species}_V.fasta".format(species=config.species),
		tab="{dir}/unique.filtered.tab.gz"
	params:
		ignore_j='--ignore-J' if config.ignore_j else ''
	threads: 4  # TODO this could be increased up to 6
	shell:
		"igdiscover discover -j {threads} --subsample {config.subsample} "
		"--database {input.v_reference} {params.ignore_j} "
		"{input.tab} > {output.tab}"


rule igdiscover_compose:
	"""Construct a new database out of the discovered sequences"""
	output:
		fasta='iteration-{nr}/new_V_database.fasta'
	input:
		tab='iteration-{nr}/candidates.tab'
	run:
		nr = int(wildcards.nr, base=10)
		criteria = '--looks-like-V --max-differences=2 '
		if nr == config.iterations:
			# Apply stricter filtering criteria for final iteration
			criteria += '--unique-CDR3=3 --consensus-seqs=200'
		else:
			criteria += '--unique-CDR3=2'
		shell("igdiscover compose {criteria} {input.tab} > {output.fasta}")


rule stats_correlation_V_J:
	output:
		pdf="{base}.correlationVJ.pdf"
	input:
		table="{base}.assigned.tab.gz"
	run:
		import matplotlib
		matplotlib.use('pdf')
		# sns.heatmap will not work properly with the object-oriented interface,
		# so use pyplot
		import matplotlib.pyplot as plt
		import seaborn as sns
		import numpy as np
		import pandas as pd
		from collections import Counter
		table = igdiscover.read_table(input.table)
		fig = plt.figure(figsize=(29.7/2.54, 21/2.54))
		counts = np.zeros((21, 11), dtype=np.int64)
		counter = Counter(zip(table['V_errors'], table['J_errors']))
		for (v,j), count in counter.items():
			if v is not None and v < counts.shape[0] and j is not None and j < counts.shape[1]:
				counts[v,j] = count
		df = pd.DataFrame(counts.T)[::-1]
		df.index.name = 'J errors'
		df.columns.name = 'V errors'
		sns.heatmap(df, annot=True, fmt=',d', cbar=False)
		fig.suptitle('V errors vs. J errors in unfiltered sequences')
		fig.set_tight_layout(True)
		fig.savefig(output.pdf)


rule plot_errorhistograms:
	output:
		pdf='{base}.errorhistograms.pdf',
	input:
		table='{base}.filtered.tab.gz'
	params:
		ignore_j='--ignore-J' if config.ignore_j else ''
	shell:
		'igdiscover errorplot {params.ignore_j} {input.table} {output.pdf}'


rule dendrogram:
	output:
		pdf='{dir}/{gene}_dendrogram.pdf'
	input:
		fasta='{{dir}}/database/{species}_{{gene}}.fasta'.format(species=config.species)
	shell:
		'igdiscover dendrogram --mark database/{config.species}_{wildcards.gene}.fasta {input.fasta} {output.pdf}'
